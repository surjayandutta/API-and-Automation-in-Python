{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "932ec995-dcf2-4e67-be1d-79c42b10a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting simple-ddl-parser\n",
      "  Using cached simple_ddl_parser-1.7.1-py3-none-any.whl.metadata (40 kB)\n",
      "Collecting ply<4.0,>=3.11 (from simple-ddl-parser)\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
      "Using cached simple_ddl_parser-1.7.1-py3-none-any.whl (83 kB)\n",
      "Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Installing collected packages: ply, simple-ddl-parser\n",
      "Successfully installed ply-3.11 simple-ddl-parser-1.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install simple-ddl-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2e88fd-da01-4d41-9608-5dfc3b549543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_ddl_parser import DDLParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "640ae4b1-bada-4b88-87aa-d736563ec39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddl_1 = \"\"\"create table dev.data_sync_history(\n",
    "    data_sync_id bigint not null,\n",
    "    sync_count bigint not null,\n",
    "    sync_mark timestamp  not  null,\n",
    "    sync_start timestamp  not null,\n",
    "    sync_end timestamp  not null,\n",
    "    message varchar(2000) null,\n",
    "    primary key (data_sync_id, sync_start)\n",
    ");\n",
    "CREATE TABLE orders (\n",
    "    order_id INT IDENTITY(1,1) PRIMARY KEY COMMENT 'Unique identifier for an order',\n",
    "    customer_id INT NOT NULL COMMENT 'Foreign key referencing customers table',\n",
    "    order_date DATE NOT NULL COMMENT 'Date the order was placed',\n",
    "    total_amount DECIMAL(10,2) NOT NULL COMMENT 'Total amount of the order' default 0,\n",
    "    CONSTRAINT fk_orders_customers FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
    ");\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc63097-e738-432c-8286-570582f23db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_results = DDLParser(ddl_1).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "172608c1-356b-4adc-9bab-2b3965fb5b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'table_name': 'data_sync_history',\n",
       "  'schema': 'dev',\n",
       "  'primary_key': ['data_sync_id', 'sync_start'],\n",
       "  'columns': [{'name': 'data_sync_id',\n",
       "    'type': 'bigint',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None},\n",
       "   {'name': 'sync_count',\n",
       "    'type': 'bigint',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None},\n",
       "   {'name': 'sync_mark',\n",
       "    'type': 'timestamp',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None},\n",
       "   {'name': 'sync_start',\n",
       "    'type': 'timestamp',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None},\n",
       "   {'name': 'sync_end',\n",
       "    'type': 'timestamp',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None},\n",
       "   {'name': 'message',\n",
       "    'type': 'varchar',\n",
       "    'size': 2000,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': True,\n",
       "    'default': None,\n",
       "    'check': None}],\n",
       "  'alter': {},\n",
       "  'checks': [],\n",
       "  'index': [],\n",
       "  'partitioned_by': [],\n",
       "  'tablespace': None},\n",
       " {'table_name': 'orders',\n",
       "  'schema': None,\n",
       "  'primary_key': ['order_id'],\n",
       "  'columns': [{'name': 'order_id',\n",
       "    'type': 'INT',\n",
       "    'size': None,\n",
       "    'identity': (1, 1),\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None,\n",
       "    'comment': \"'Unique identifier for an order'\"},\n",
       "   {'name': 'customer_id',\n",
       "    'type': 'INT',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None,\n",
       "    'comment': \"'Foreign key referencing customers table'\"},\n",
       "   {'name': 'order_date',\n",
       "    'type': 'DATE',\n",
       "    'size': None,\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': None,\n",
       "    'check': None,\n",
       "    'comment': \"'Date the order was placed'\"},\n",
       "   {'name': 'total_amount',\n",
       "    'type': 'DECIMAL',\n",
       "    'size': (10, 2),\n",
       "    'references': None,\n",
       "    'unique': False,\n",
       "    'nullable': False,\n",
       "    'default': 0,\n",
       "    'check': None,\n",
       "    'comment': \"'Total amount of the order'\"}],\n",
       "  'alter': {},\n",
       "  'checks': [],\n",
       "  'index': [],\n",
       "  'partitioned_by': [],\n",
       "  'constraints': {'references': [{'table': 'customers',\n",
       "     'columns': ['customer_id'],\n",
       "     'schema': None,\n",
       "     'on_delete': None,\n",
       "     'on_update': None,\n",
       "     'deferrable_initially': None,\n",
       "     'name': 'customer_id',\n",
       "     'constraint_name': 'fk_orders_customers'}]},\n",
       "  'tablespace': None}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a156cf-b96c-4823-be6f-455e4e806fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ddlparse\n",
      "  Using cached ddlparse-1.10.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (3.1.5)\n",
      "Collecting pyparsing (from ddlparse)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Using cached ddlparse-1.10.0-py3-none-any.whl (10 kB)\n",
      "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "Installing collected packages: pyparsing, ddlparse\n",
      "Successfully installed ddlparse-1.10.0 pyparsing-3.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ddlparse openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac6b43e9-3d6e-463d-9c06-39fb5c3bf235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a923a4d9-7d18-4d17-a246-b8e9c0094786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\surjayandutta\\anaconda3\\envs\\syntheticos\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f9d7b8f-d4d8-4394-9d43-5c2e89a8f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ae4c235-b15f-4363-98ce-3fbfaa688327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_dataframe_from_sql(sql_file_path):\n",
    "    try:\n",
    "        # Step 1: Read SQL content\n",
    "        with open(sql_file_path, 'r') as file:\n",
    "            ddl_content = file.read()\n",
    "\n",
    "        # Step 2: Parse SQL using DDLParser\n",
    "        parser = DDLParser(ddl_content)\n",
    "        parsed_data = parser.run()  # Returns a list of table definitions\n",
    "\n",
    "        if not parsed_data:\n",
    "            raise ValueError(\"Parsing failed. Ensure SQL syntax is correct.\")\n",
    "\n",
    "        # Step 3: Create an empty pandas DataFrame with the required columns\n",
    "        columns = [ \"TableName\", \"Columns\", \"DataFamily\", \"MinValue\", \"MaxValue\", \"CustomDirectListValue\", \"fk_TableName\", \"fk_ColumnName\", \"Description\"]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Step 4: Populate the DataFrame with parsed data\n",
    "        for table_info in parsed_data:\n",
    "            table_name = table_info[\"table_name\"]\n",
    "            fk_constraints = table_info.get(\"constraints\", {}).get(\"references\", [])\n",
    "\n",
    "            for column in table_info[\"columns\"]:\n",
    "                # Default foreign key info to None\n",
    "                fk_table = None\n",
    "                fk_column = None\n",
    "\n",
    "                # Check if the column is part of any foreign key constraint\n",
    "                for fk in fk_constraints:\n",
    "                    if column[\"name\"] in fk[\"columns\"]:\n",
    "                        fk_table = fk[\"table\"]\n",
    "                        fk_column = fk[\"columns\"][0]  # Assuming single-column foreign keys\n",
    "                        break\n",
    "\n",
    "                # Get the description/comment\n",
    "                description = column.get(\"comment\", \"\")\n",
    "                data_family = \"Unknown\"  # Default or derived value\n",
    "                min_value = None  # Populate based on column type or constraints\n",
    "                max_value = None  # Populate based on column type or constraints\n",
    "                custom_direct_list_value = None  # Populate based on specific rules\n",
    "\n",
    "                # Add a row to the DataFrame\n",
    "                df = pd.concat([df, pd.DataFrame([{\n",
    "                \"TableName\": table_name,\n",
    "                \"Columns\": column[\"name\"],\n",
    "                \"DataFamily\": data_family,\n",
    "                \"MinValue\": min_value,\n",
    "                \"MaxValue\": max_value,\n",
    "                \"CustomDirectListValue\": custom_direct_list_value,\n",
    "                \"fk_TableName\": fk_table,\n",
    "                \"fk_ColumnName\": fk_column,\n",
    "                \"Description\": description\n",
    "                }])], ignore_index=True)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "# df = generate_dataframe_from_sql(\"input.sql\")\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bbfb671-2819-4c1c-b48d-d88280a680e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TableName</th>\n",
       "      <th>Columns</th>\n",
       "      <th>DataFamily</th>\n",
       "      <th>MinValue</th>\n",
       "      <th>MaxValue</th>\n",
       "      <th>CustomDirectListValue</th>\n",
       "      <th>fk_TableName</th>\n",
       "      <th>fk_ColumnName</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>data_sync_id</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>sync_count</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>sync_mark</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>sync_start</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>sync_end</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data_sync_history</td>\n",
       "      <td>message</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>orders</td>\n",
       "      <td>order_id</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>'Unique identifier for an order'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>orders</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>customers</td>\n",
       "      <td>customer_id</td>\n",
       "      <td>'Foreign key referencing customers table'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>orders</td>\n",
       "      <td>order_date</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>'Date the order was placed'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>orders</td>\n",
       "      <td>total_amount</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>'Total amount of the order'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TableName       Columns DataFamily MinValue MaxValue  \\\n",
       "0  data_sync_history  data_sync_id    Unknown     None     None   \n",
       "1  data_sync_history    sync_count    Unknown     None     None   \n",
       "2  data_sync_history     sync_mark    Unknown     None     None   \n",
       "3  data_sync_history    sync_start    Unknown     None     None   \n",
       "4  data_sync_history      sync_end    Unknown     None     None   \n",
       "5  data_sync_history       message    Unknown     None     None   \n",
       "6             orders      order_id    Unknown     None     None   \n",
       "7             orders   customer_id    Unknown     None     None   \n",
       "8             orders    order_date    Unknown     None     None   \n",
       "9             orders  total_amount    Unknown     None     None   \n",
       "\n",
       "  CustomDirectListValue fk_TableName fk_ColumnName  \\\n",
       "0                  None         None          None   \n",
       "1                  None         None          None   \n",
       "2                  None         None          None   \n",
       "3                  None         None          None   \n",
       "4                  None         None          None   \n",
       "5                  None         None          None   \n",
       "6                  None         None          None   \n",
       "7                  None    customers   customer_id   \n",
       "8                  None         None          None   \n",
       "9                  None         None          None   \n",
       "\n",
       "                                 Description  \n",
       "0                                             \n",
       "1                                             \n",
       "2                                             \n",
       "3                                             \n",
       "4                                             \n",
       "5                                             \n",
       "6           'Unique identifier for an order'  \n",
       "7  'Foreign key referencing customers table'  \n",
       "8                'Date the order was placed'  \n",
       "9                'Total amount of the order'  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dataframe_from_sql(\"CREATE_TABLE_dev.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92439341-7e6f-40bf-b6d0-d04a7c66080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_sql_file(uploaded_file):\n",
    "    \"\"\"\n",
    "    Accept an uploaded .xlsx, .sql, or .ddl file, process it, filter by distinct table names, \n",
    "    and return each as a JSON response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process the .sql or .ddl file\n",
    "    # Step 1: Read the file content\n",
    "    with open(uploaded_file, 'r') as file:\n",
    "            contents = await file.read()\n",
    "    \n",
    "    ddl_content = contents.decode('utf-8')  # Decode bytes to string\n",
    "    logger.info(\"sql file is loaded\")\n",
    "    # Step 2: Parse the SQL using DDLParser\n",
    "    parser = DDLParser(ddl_content)\n",
    "    parsed_data = parser.run()\n",
    "    logger.info(\"sql data parsed\")\n",
    "\n",
    "    # Step 3: Create a pandas DataFrame with the required structure\n",
    "    columns = [\"TableName\", \"Columns\", \"fk_TableName\", \"fk_ColumnName\", \"Description\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Step 4: Populate the DataFrame\n",
    "    for table_info in parsed_data:\n",
    "        table_name = table_info[\"table_name\"]\n",
    "        fk_constraints = table_info.get(\"constraints\", {}).get(\"references\", [])\n",
    "\n",
    "        for column in table_info[\"columns\"]:\n",
    "            # Default foreign key info to None\n",
    "            fk_table = None\n",
    "            fk_column = None\n",
    "\n",
    "            # Check if the column is part of any foreign key constraint\n",
    "            for fk in fk_constraints:\n",
    "                if column[\"name\"] in fk[\"columns\"]:\n",
    "                    fk_table = fk[\"table\"]\n",
    "                    fk_column = fk[\"columns\"][0]  # Assuming single-column foreign keys\n",
    "                    break\n",
    "\n",
    "            # Get the description/comment\n",
    "            description = column.get(\"comment\", \"\")\n",
    "\n",
    "            # Add a row to the DataFrame\n",
    "            df = pd.concat([df, pd.DataFrame([{\n",
    "                \"TableName\": table_name,\n",
    "                \"Columns\": column[\"name\"],\n",
    "                \"fk_TableName\": fk_table,\n",
    "                \"fk_ColumnName\": fk_column,\n",
    "                \"Description\": description\n",
    "            }])], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d13cfe28-65f5-433a-8958-0e9f219f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from fastapi import HTTPException\n",
    "\n",
    "async def process_file(uploaded_file):\n",
    "    \"\"\"\n",
    "    Accept an uploaded .xlsx, .sql, or .ddl file, process it, filter by distinct table names, \n",
    "    and return each as a JSON response.\n",
    "    \"\"\"\n",
    "    if uploaded_file.filename.endswith('.xlsx'):\n",
    "        # Load the uploaded .xlsx file into a DataFrame\n",
    "        contents = await uploaded_file.read()\n",
    "        df = pd.read_excel(io.BytesIO(contents), sheet_name=\"ColumnInfo\", keep_default_na=False)\n",
    "\n",
    "        # Check if 'TableName' exists in the columns\n",
    "        if 'TableName' not in df.columns:\n",
    "            raise HTTPException(detail=\"The 'TableName' column was not found in the template.\")\n",
    "    elif uploaded_file.filename.endswith(('.sql', '.ddl')):\n",
    "        # Process the .sql or .ddl file\n",
    "        # Step 1: Read the file content\n",
    "        contents = await uploaded_file.read()\n",
    "        ddl_content = contents.decode('utf-8')  # Decode bytes to string\n",
    "\n",
    "        # Step 2: Parse the SQL using DDLParser\n",
    "        parser = DDLParser(ddl_content)\n",
    "        parsed_data = parser.run()\n",
    "\n",
    "        # Step 3: Create a pandas DataFrame with the required structure\n",
    "        columns = [\"TableName\", \"Columns\", \"fk_TableName\", \"fk_ColumnName\", \"Description\"]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "\n",
    "        # Step 4: Populate the DataFrame\n",
    "        for table_info in parsed_data:\n",
    "            table_name = table_info[\"table_name\"]\n",
    "            fk_constraints = table_info.get(\"constraints\", {}).get(\"references\", [])\n",
    "\n",
    "            for column in table_info[\"columns\"]:\n",
    "                # Default foreign key info to None\n",
    "                fk_table = None\n",
    "                fk_column = None\n",
    "\n",
    "                # Check if the column is part of any foreign key constraint\n",
    "                for fk in fk_constraints:\n",
    "                    if column[\"name\"] in fk[\"columns\"]:\n",
    "                        fk_table = fk[\"table\"]\n",
    "                        fk_column = fk[\"columns\"][0]  # Assuming single-column foreign keys\n",
    "                        break\n",
    "\n",
    "                # Get the description/comment\n",
    "                description = column.get(\"comment\", \"\")\n",
    "\n",
    "                # Add a row to the DataFrame\n",
    "                df = pd.concat([df, pd.DataFrame([{\n",
    "                    \"TableName\": table_name,\n",
    "                    \"Columns\": column[\"name\"],\n",
    "                    \"fk_TableName\": fk_table,\n",
    "                    \"fk_ColumnName\": fk_column,\n",
    "                    \"Description\": description\n",
    "                }])], ignore_index=True)\n",
    "    else:\n",
    "        # Raise an error for unsupported file types\n",
    "        raise HTTPException(detail=\"Invalid file type. Please upload an .xlsx, .sql, or .ddl file.\")\n",
    "\n",
    "    # Extract distinct table names\n",
    "    unique_table_names = df['TableName'].unique()\n",
    "\n",
    "    # Initialize the dictionary to hold filtered data for each table\n",
    "    template_data = {}\n",
    "\n",
    "    # Filter records by each table name and store in dictionary\n",
    "    for table_name in unique_table_names:\n",
    "        # Filter DataFrame for the current table name\n",
    "        filtered_df = df[df['TableName'] == table_name]\n",
    "\n",
    "        # Convert the filtered DataFrame to a list of dictionaries\n",
    "        template_data[table_name] = filtered_df.to_dict(orient=\"records\")\n",
    "\n",
    "    # Return the JSON response\n",
    "    return template_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eb78d22-cf47-4c13-b3db-897fe039f80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object process_file at 0x0000023ABEC2DD40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_file(\"CREATE TABLE dev.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a24e17a-b6f7-479a-87ad-ed509f040b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863417f-ae66-47b6-a5a9-6d35019c71f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb88c87-64e3-49c0-9fa4-b708fefd5e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee0b683-c2cb-42cc-a71e-3bf2ab460b06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb57bf-4dd8-4302-afab-a08300a34f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5f542b-9084-4b6a-bd98-3e46f3df23de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea6f25c-8824-4d6c-8e8d-6220a39b18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openpyxl import Workbook\n",
    "import re\n",
    "\n",
    "def generate_template_from_sql(sql_file_path, output_excel_path):\n",
    "    # Step 1: Read SQL content\n",
    "    with open(sql_file_path, 'r') as file:\n",
    "        ddl_content = file.read()\n",
    "\n",
    "    # Step 2: Extract all table names using a regular expression\n",
    "    table_names = re.findall(r'CREATE TABLE\\s+`?(\\w+(\\.\\w+)?)`?\\s*\\(', ddl_content, re.IGNORECASE)\n",
    "    table_names = [name[0] for name in table_names]  # Flatten the list to just table names\n",
    "\n",
    "    # Step 3: Parse SQL using DDLParser\n",
    "    parser = DDLParser(ddl_content)\n",
    "    parsed_data = parser.run()  # This returns a list, not a dict\n",
    "\n",
    "    # Step 4: Create a new Excel file and add \"ColumnInfo\" sheet\n",
    "    workbook = Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.title = \"ColumnInfo\"\n",
    "\n",
    "    # Step 5: Add headers to the \"ColumnInfo\" sheet\n",
    "    headers = [\"TableName\", \"Columns\", \"fk_TableName\", \"fk_ColumnName\", \"Description\"]\n",
    "    sheet.append(headers)\n",
    "\n",
    "    # Step 6: Populate the \"ColumnInfo\" sheet with parsed data\n",
    "    table_index = 0  # To keep track of which table we're currently processing\n",
    "    for table_info in parsed_data:\n",
    "        # Get the correct table name from the list\n",
    "        table_name = table_names[table_index]\n",
    "        fk_constraints = table_info.get(\"constraints\", {}).get(\"references\", [])\n",
    "\n",
    "        for column in table_info[\"columns\"]:\n",
    "            # Default foreign key info to None\n",
    "            fk_table = None\n",
    "            fk_column = None\n",
    "\n",
    "            # Check if the column is part of any foreign key constraint\n",
    "            for fk in fk_constraints:\n",
    "                if column[\"name\"] in fk[\"columns\"]:\n",
    "                    fk_table = fk[\"table\"]\n",
    "                    fk_column = fk[\"columns\"][0]  # Assuming single-column foreign keys for now\n",
    "                    break\n",
    "\n",
    "            # Get the description/comment\n",
    "            description = column.get(\"comment\", \"\")\n",
    "\n",
    "            # Append row data with the correct table name\n",
    "            sheet.append([table_name, column[\"name\"], fk_table, fk_column, description])\n",
    "\n",
    "        # Move to the next table name in the list\n",
    "        table_index += 1\n",
    "\n",
    "    # Step 7: Save the new Excel file\n",
    "    workbook.save(output_excel_path)\n",
    "    print(f\"Excel file created: {output_excel_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a73ccf9-d0b5-4a1f-9ba2-8abf3575b69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created: New_Output1.xlsx\n"
     ]
    }
   ],
   "source": [
    "generate_template_from_sql(\"create_new_table.txt\",  \"New_Output1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf196d-3148-4cae-a7db-ab031ff61db9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syntheticos",
   "language": "python",
   "name": "syntheticos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
